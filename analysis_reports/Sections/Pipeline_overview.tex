\section{Pipeline Overview}

The pipeline has two entry points: text (\texttt{POST /tts}) and speech (\texttt{POST /sts}). Both converge to the same processing chain after obtaining a user message.

\subsection{Text Path (\texttt{/tts})}
The server receives \texttt{message} in the body. It first checks for default responses (empty message or missing API keys) by reading pre-generated audio and JSON from disk. If no default applies, it loads a structured-output parser and the OpenAI chain (including party program from \texttt{content/party\_program.md}), invokes the chain to get up to three messages (text, facial expression, animation), then runs the lip-sync stage and returns the result.

\subsection{Speech Path (\texttt{/sts})}
The server receives base64-encoded audio. It decodes to a buffer, converts WebM to MP3 via ffmpeg in \texttt{utils/audios.mjs}, writes the MP3 to a temp file, and calls the OpenAI Whisper API for transcription. The resulting text is then processed exactly like the text path (default check, OpenAI chain, lip-sync).

\subsection{Lip-Sync Stage}
For each message returned by the LLM:
\begin{enumerate}
    \item \textbf{TTS}: ElevenLabs \texttt{textToSpeech} is called (with retries on HTTP 429). Audio is written to \texttt{audios/message\_$i$.mp3}.
    \item \textbf{Phonemes}: The MP3 is converted to WAV with ffmpeg, then Rhubarb Lip-Sync is run to produce \texttt{message\_$i$.json} with mouth cues.
    \item \textbf{Envelope}: The MP3 is read and base64-encoded; the JSON transcript is read and attached to the message.
\end{enumerate}
TTS is done in parallel across messages (\texttt{Promise.all}); phoneme generation and file reads are also done in parallel across messages, but only \emph{after} all TTS calls complete.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=1.4cm, block/.style={rectangle, draw, fill=blue!15, rounded corners, minimum width=2.2cm, minimum height=0.7cm, align=center, font=\small}]
    \node [block] (tts) {\texttt{/tts}\\Text};
    \node [block, below left=1.6cm and 1.2cm of tts] (sts) {\texttt{/sts}\\Speech};
    \node [block, below=1.2cm of tts] (whisper) {Whisper\\+ ffmpeg};
    \node [block, below=0.9cm of whisper] (default) {Default\\check};
    \node [block, below=0.9cm of default] (openai) {OpenAI\\chain};
    \node [block, below=0.9cm of openai] (lipsync) {Lip-sync\\(TTS + phonemes)};
    \node [block, below=0.9cm of lipsync] (out) {Response};
    \draw [->, thick] (tts) -- (default);
    \draw [->, thick] (sts) -- (whisper);
    \draw [->, thick] (whisper) -- (default);
    \draw [->, thick] (default) -- (openai);
    \draw [->, thick] (openai) -- (lipsync);
    \draw [->, thick] (lipsync) -- (out);
\end{tikzpicture}
\caption[Entry points and pipeline]{Two entry points (\texttt{/tts} and \texttt{/sts}); speech path adds Whisper and ffmpeg before joining the common chain.}
\label{fig:entry_points}
\end{figure}

Figure~\ref{fig:pipeline} in the appendix summarises the full flow.
