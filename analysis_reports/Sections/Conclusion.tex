\section{Conclusion}

The avatar pipeline is clear and modular but has several weaknesses in time, spending, and bottlenecks.

\textbf{Time}: Latency is dominated by the sequential chain (OpenAI then lip-sync) and by the lip-sync stage (TTS then phonemes) with no streaming. Caching the party program and reusing the chain more aggressively would reduce per-request work; streaming the first message while generating the rest would improve perceived latency.

\textbf{Spending}: There is no caching of LLM or TTS outputs, and no tiering by query complexity. Every \texttt{/sts} call uses Whisper; every non-default reply uses ElevenLabs for up to three messages. Caching and single-message or cheaper-model options would reduce cost.

\textbf{Bottlenecks}: ElevenLabs rate limits (with zero backoff) and the strict TTS-then-phonemes ordering make TTS the main bottleneck. Shared \texttt{audios} filenames across requests risk cross-request overwrites and incorrect responses; per-request or per-session directories would fix this. Process spawning for ffmpeg and Rhubarb could be capped or queued under high load.

Addressing the shared-audio filenames and adding minimal caching and backoff would improve correctness and efficiency with limited changes; streaming and cost controls would require broader pipeline changes but would yield the largest gains in user experience and cost.
