TEMA 1c: Hvordan gjennomføre eksamener når AI er tilgjengelig? Hva er det som skal vurderes?
A. Standpunkt (1–2 setninger)
Teknologisk folkeparti vil reformere eksamen fra et “anti-AI-kontrollregime” til et kompetanseregime: AI kan ofte være tillatt, men studentens forståelse må bevises gjennom sporbar prosess, verifikasjon og forsvar. Eksamen skal måle det AI ikke kan garantere: faglig dømmekraft, metode, etikk, kvalitetssikring og evne til å anvende kunnskap i nye situasjoner.
Bouvet-intro-og-oppgaver

B. Hvorfor (kjerneargument, skrevet som sammenhengende tekst)
Når AI er tilgjengelig for alle, blir det en grunnleggende feil å late som om eksamen kan “skjermes” ved å stramme til reglene. Totalforbud gir i praksis et system der de mest ressurssterke eller mest risikovillige lærer seg å skjule bruken best, mens de mest samvittighetsfulle straffes. Det skaper et urettferdig og konfliktfylt læringsmiljø, og det flytter oppmerksomheten fra læring til kontroll. Teknologisk folkeparti mener derfor at eksamen må bygges slik at den tåler at AI finnes – på samme måte som vi i andre tider har bygget vurderingsformer som tåler at kalkulatorer, internett, digitale verktøy og avanserte hjelpemidler er en del av virkeligheten.
Kjernen i reformen er et skifte fra å vurdere “produktet” til å vurdere “prosessen”. I en AI-tid er sluttproduktet ofte mindre informativt enn før. En tekst kan se korrekt ut uten at studenten forstår den, en kode kan fungere uten at studenten kan forklare hvorfor, og en rapport kan være språklig sterk uten at metode og resonnement holder. Derfor må eksamen i større grad kreve bevis på kompetanse: hvilke valg som ble tatt, hvilke antakelser som ble lagt inn, hvordan kandidaten testet påstander og resultater, hvilke feilkilder som ble vurdert, og hva kandidaten gjorde når verktøyet tok feil. Utdanningsdirektoratet peker på at når KI er lett tilgjengelig, blir det særlig viktig å variere vurderingsformer og bygge vurdering på flere kilder – nettopp for å sikre at det er elevens kompetanse som vurderes. Prinsippet er det samme i høyere utdanning og fagutdanning: eksamen må gi flere “vinduer” inn i kandidatens forståelse.
Samtidig må eksamen speile arbeidslivet. I de fleste profesjoner er det ikke et mål å jobbe uten verktøy; målet er å jobbe riktig, trygt og etterprøvbart med verktøy. Det vi bør teste, er derfor ikke om kandidaten klarer å isolere seg fra teknologi, men om kandidaten kan bruke teknologi under ansvar: om de kan avgrense bruken, håndtere usikkerhet, vurdere etikk og personvern, og stå inne for resultatet. Et slikt regime er mer realistisk, og det bygger bedre tillit til kompetansen enn et regime som i praksis belønner skjult bruk eller ren memorering.
I en AI-tid blir “verifikasjonsevne” en kjernekompetanse på tvers av fag. Generativ AI kan gi plausible feil, blande regelverk, hoppe over viktige forutsetninger, eller presentere overfladisk tekst som ser faglig ut. Dermed må eksamen i større grad måle evnen til å oppdage feil, sjekke kilder, gjøre kontrollregning, teste robusthet og vurdere om en løsning faktisk passer konteksten. Det er nettopp denne typen kompetanse som skiller en trygg fagperson fra en som bare kan gjengi.
Vi må også ta innover oss at et desperat “anti-juks”-regime lett glir over i et overvåkningsregime. Fjernproktorering, aggressiv logging, ansiktsanalyse, “AI-detektorer” og lignende tiltak skaper store personvernutfordringer og kan gi falske positive som rammer uskyldige. Teknologisk folkeparti vil heller bygge vurdering som er robust i seg selv – muntlig forsvar, praktisk demonstrasjon, casebasert resonnering og dokumentert prosess – enn å bygge en eksamen som krever stadig mer inngripende kontroll for å fungere.
Til slutt må vi være tydelige på sensur: AI kan være nyttig som støtte (for struktur, sjekklister, anonymisering og konsistenskontroll), men bruk av AI som “dommer” eller som automatisk karaktersetter er risikabelt. Debatten om AI-støtte i sensur viser hvor lett tilliten kan svekkes hvis det ikke er full transparens og menneskelig ansvar. Teknologisk folkepartis linje er derfor enkel: AI kan støtte, men aldri dømme.

C. Konkret politikk (6–10 tiltak – skrevet som tekst med mål, hvordan, ansvar)
Kort sikt (0–2 år): Vi etablerer et nytt minimumsregime for eksamen. Først innfører vi et nasjonalt prinsipp om at vurdering i større emner ikke skal hvile på én sårbar sluttinnlevering alene, men bygge på minst to uavhengige vurderingskilder. Målet er rettferdighet og robusthet: når AI kan påvirke produkter, må vi også måle forståelse gjennom andre kanaler. Hvordan gjør vi det i praksis? Ved å standardisere kombinasjoner som “skriftlig produkt + muntlig miniforsvar”, “caseoppgave + kort kontrollprøve”, eller “prosjekt + praktisk demonstrasjon”. Ansvaret ligger hos staten for å sette rammekrav (minimumsprinsipp), og hos institusjonene/skolene for å implementere lokalt.
Deretter standardiserer vi en enkel, nasjonal mal for AI-logg når AI er tillatt i eksamen. Målet er ikke å “ta” studenter, men å gjøre arbeidsprosessen etterprøvbar. Loggen skal være kort og praktisk: hvilket verktøy som ble brukt, til hva, hva som ble forkastet, hva som ble kontrollert, og hvilke kilder/ tester som underbygger konklusjonene. Trafikklysmodeller som HVL har utviklet, viser hvordan dokumentasjonskrav kan gjøres konkret og forståelig for studenter. Ansvaret ligger hos institusjonene for å kreve logg i emnebeskrivelser og eksamensinstruks, mens staten setter minimumsmal og sikrer at praksis blir konsistent.
Vi gjør også muntlig miniforsvar til standard ved større innleveringer. Målet er enkelt: å sikre at kandidaten kan forklare og stå for arbeidet uavhengig av verktøy. Hvordan? Gjennom en kort, strukturert samtale (8–15 minutter) der studenten må (1) forklare to–tre nøkkelvalg, (2) gjøre én endring på stående fot (“hva hvis forutsetningen endres?”), og (3) identifisere en svakhet eller usikkerhet i eget arbeid. Dette skaper et direkte “bevis på forståelse” uten at vi trenger inngripende overvåkningsmetoder. Ansvaret ligger hos institusjonene og fagmiljøene, som også utvikler rubrikker for lik vurdering.
Til slutt i kort sikt etablerer vi en “AI-sandkasse” for eksamen: godkjente verktøy med tydelige rammer, slik at vi unngår “vill vest”. Målet er lik tilgang og bedre personvern. Hvordan? Skolen/institusjonen tilbyr godkjente verktøy i eksamensmodus, med klare sperrer mot å legge inn sensitiv informasjon, og med transparens rundt hvilken logging som skjer. Logging skal være til etterprøvbarhet, ikke overvåkning, og studenten skal kunne se hva som er logget. Ansvaret er delt mellom institusjon (anskaffelse og drift) og stat (standarder og støtte).
Middels sikt (2–6 år): Vi bygger selve eksamensdesignen om. Hovedgrepet her er “to-spors eksamen” i kjerneemner. Målet er å måle to ting samtidig: grunnforståelse og profesjonell verktøybruk. Hvordan? Del A blir en tidsavgrenset kjerneprøve med begrensede hjelpemidler der begreper, metode og resonnering testes. Del B blir en casebasert profesjonsoppgave der AI kan være tillatt, men med krav om verifikasjon, dokumentasjon og et muntlig/praktisk forsvar. Dette gir en robust kompetanseprofil: kandidaten viser både at de kan faget uten verktøyhjelp, og at de kan bruke verktøy ansvarlig. Ansvaret ligger hos institusjonene for design, og hos staten for å finansiere utviklingsarbeid og sikre nasjonal rettferdighet.
Vi går også systematisk fra generiske oppgaver til autentiske case med unike premisser. Målet er å gjøre eksamen mindre “outsourcbar” og mer faglig meningsfull. Hvordan? Ved å bruke ukjente datasett, lokale standarder, konkrete målinger, reelle problemstillinger fra praksisfeltet, eller scenarioer som krever faglig skjønn. AI kan bidra med forslag, men kan ikke erstatte kontekstforståelse og kvalitetssikring. Ansvaret ligger hos fagmiljøene (oppgaveutvikling) og institusjonene (kvalitet og sensur).
I fag der prosess er kompetansen – programmering, design, prosjekt, laboratoriearbeid – innfører vi portefølje og versjonshistorikk som del av vurderingen. Målet er å vurdere utvikling, beslutninger og testkultur, ikke bare sluttresultat. Hvordan? Gjennom repo/versjonshistorikk, lab-notat, testlogg og refleksjon, kombinert med en kort demonstrasjon der kandidaten må forklare og endre løsningen. Ansvaret ligger hos institusjonene.
Lang sikt (6–15 år): Vi gjør moderniseringen permanent og nasjonalt koordinert. Her etablerer vi nasjonale program for oppgaveutvikling, sensoropplæring og deling av vurderingsrubrikker, slik at reformen ikke blir fragmentert og urettferdig. Utdanningsdirektoratet har allerede gjort konkrete endringer i eksamensopplegg som følge av økt KI-bruk, noe som viser at systemet kan endres når virkeligheten endrer seg. Prinsippet må videreføres: eksamen skal designes ut fra hva vi ønsker å måle – ikke ut fra hva som er lettest å kontrollere.
Vi innfører også tydelige nasjonale krav for AI i sensur: “støtte, ikke dom”. Målet er å beskytte tillit og rettssikkerhet. Hvordan? AI kan brukes til teknisk støtte, men ikke til å rangere eller gi karakterforslag uten strenge krav, transparens og revisjon. Ansvaret ligger hos staten for regelverk og tilsyn, og hos institusjonene for etterlevelse.

D. Risiko og sikkerhetsnett (skrevet som tekst)
Den største risikoen er at AI blir en erstatning for grunnleggende ferdigheter og at “kjernekompetansen” forvitrer. Sikkerhetsnettet er to-spors vurdering og bevisst progresjon: noen deler av eksamen måler grunnforståelse uten AI, mens andre måler profesjonell bruk med AI – med krav om verifikasjon og forklaring. Slik får vi både dybde og relevans.
En annen risiko er urettferdighet: ulik tilgang til verktøy, uklare regler eller varierende praksis mellom faglærere. Sikkerhetsnettet er institusjonsgodkjente verktøy (lik tilgang), trafikklysregler per emne, og standardisert AI-logg som gjør praksis etterprøvbar og forståelig.
En tredje risiko er personvernbrudd og økende overvåkning gjennom fjernproktorering og aggressiv datainnsamling. Sikkerhetsnettet er å velge vurderingsformer som er robuste uten invasive kontrolltiltak: muntlig forsvar, praktisk prøve, flere vurderingskilder og autentiske case. Der logging brukes, skal den være transparent, begrenset og formålsstyrt.
En fjerde risiko er falske anklager basert på “AI-detektorer” som ikke er pålitelige. Sikkerhetsnettet er klare rettssikkerhetskrav: automatdeteksjon kan aldri være eneste grunnlag. Mistanke må håndteres gjennom faglig oppfølging – muntlig redegjørelse, prosessgjennomgang og dokumentasjon.
Til slutt er det en reell risiko for at AI i sensur svekker tillit, særlig hvis studenter opplever at “maskinen” vurderer dem. Sikkerhetsnettet er prinsippet om menneskelig ansvar og full transparens: AI kan støtte, men sensoren eier vurderingen og må kunne begrunne den.

E. Måling og ansvarliggjøring (3–6 KPI-er, i tekst)
For å vite om reformen fungerer, må vi måle både rettferdighet, læring og tillit. Vi skal derfor følge studentenes opplevelse av klarhet og lik praksis på tvers av emner, og vi skal måle utviklingen i fuskesaker – særlig hvor mange som skyldes uklare regler fremfor reell uredelighet. Vi skal også måle læringskvalitet i kjerneemner (stryk/frafall) etter innføring av to-spors eksamen og miniforsvar, og vi skal følge rubrikk-score på verifikasjon, metode og evne til å forklare og endre løsninger under press. Til slutt må personvernavvik knyttet til eksamensverktøy være en tydelig indikator: et system som krever overvåkning for å fungere, er ikke et system vi vil ha.

F. One-liner (maks 20 ord)
Når AI finnes, må eksamen måle dømmekraft – ikke bare tekstproduksjon.

G. Vanskeligste motargument + vårt beste svar (steelman, utdypet som tekst)
Motargumentet er at hvis AI tillates i eksamen, kan vi aldri være sikre på hva studenten kan. Da vil grader og karakterer miste verdi, fordi kandidaten kan “låne” intelligens fra maskinen. I tillegg frykter man at det blir urettferdig: den som er best til å bruke AI, får fordeler som ikke handler om faglig forståelse.
Vårt beste svar er at dette motargumentet blir riktig bare hvis vi fortsetter å vurdere det AI lett kan produsere: generisk tekst, standardiserte forklaringer og tilsynelatende pene leveranser. Teknologisk folkepartis reform gjør det motsatte: vi flytter vurderingen til områder der AI ikke kan garantere kvalitet uten menneskelig kompetanse. Når studenten må levere en sporbar prosess (AI-logg), dokumentere verifikasjon (kilder, tester, kontroll), og i tillegg forsvare og endre løsningen muntlig eller praktisk, blir det ikke enklere å skjule manglende forståelse – det blir vanskeligere. En som ikke forstår, vil falle gjennom når de må forklare antakelser, identifisere feilkilder, håndtere en endring i premissene, eller begrunne et etisk valg.
Dessuten er åpen regulering mer rettferdig enn forbud i en verden der verktøyet uansett er i bruk. Et forbudsregime belønner skjult praksis og ujevn håndheving. Et kompetanseregime belønner åpenhet og dokumentert forståelse. Da blir AI ikke en snarvei, men en stresstest: klarer kandidaten å bruke verktøyet som en profesjonell – og å ta ansvar når verktøyet tar feil? Hvis svaret er ja, er graden mer relevant for arbeidslivet. Hvis svaret er nei, vil vurderingen fange det opp nettopp fordi vi måler prosess, verifikasjon og forsvar – ikke bare sluttprodukt.

